- To Do
map, flatMap, reduce function, broadcast variable, accumulator,


----------------------------------------
Spark doesn’t provide storage layer

RDD
myrdd = sc.parallelize([1,2,3,4,5])
an RDD is just mapping of actual data and partitions.

defaultParallelism - By default it is number of cores available to application.

reduce() – executes the function passed again and again until only one value is left. The function should take two arguments and return one value.
pyspark.Accumulator - A shared variable that can be accumulated, 

- Best Practices
spark.conf.set("spark.microsoft.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.microsoft.delta.autoCompact.enabled", "true")

ref - https://learn.microsoft.com/en-us/azure/databricks/optimizations/auto-optimize



To Read:
https://mkoptelov.medium.com/unit-testing-of-databricks-notebooks-54c95631125c




